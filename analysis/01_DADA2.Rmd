---
title: "Infer ASVs with DADA2"
author: "Erica Sawyer"
output: html_document
  toc: yes
  toc_float:
    collapsed: no
    smooth_scroll: yes
    toc_depth: 3
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.path = "/local/workdir/es986/git_repos/SalinityGradient_16S/figures/01_DADA2/")
```


# Goals of this File

1. Use raw fastqc and generate quality plots to assess quality of reads
2. Filter and trim bad sequences and bases from out sequencing files
3. Write out fastqs with high quality sequences
4. Evaluate quality from filter and trim
5. Infer error on forward and reverse reads individually
6. Identify ASVs from forward and reverse reads separately
7. Merge forward and reverse ASVs into "contiguous" ASVs
8. Generate the ASV count table,(`otu_table` input for phyloseq)


Output that we need:

1. ASV count table `out_table`
2. Taxonomy Table `tax_table`
3. Sample Information `sample_data`: track the reads lost throughout the DADA2 workflow

#Before you start
```{r Set Seed}
# Any number can be chosen
set.seed(958430)
```

# load libraries
``` {r load libraries}

#install.packages("devtools")
library(devtools)

#devtools::install_github("benjjneb/dada2")
library(dada2)

#install.packages("tidyverse")
library(tidyverse)

```

# Load Data
``` {r Load Data}
# Set the raw fastq path to the raw sequencing files
# Path to the fastq files

raw_fastq_path <- "/local/workdir/es986/git_repos/SalinityGradient_16S/data/01_DADA2/01_raw_gzipped_fastqs"

# what files in this path
list.files(raw_fastq_path)

# how many files are there
str(list.files(raw_fastq_path))

#create vector or forward reads
forward_reads <- list.files(raw_fastq_path, pattern = "R1_001.fastq.gz", full.names = TRUE)
#intuition check
head(forward_reads)

#create vector of reverse reads
reverse_reads <- list.files(raw_fastq_path, pattern = "R2_001.fastq.gz", full.names = TRUE)
#intuition check
head(reverse_reads)

```

#Raw Quality Plots
```{r raw quality plot}
# randomly select two samples from dataset to evaluate

random_samples <- sample(1:length(reverse_reads), size=2)

# calculate quality and plot of these two samples
plotQualityProfile(forward_reads[random_samples]) +
  labs(title="Forward Read Raw Quality")

plotQualityProfile(reverse_reads[random_samples]) +
  labs(title="Reverse Read Raw Quality")


```

# prepare a placeholder for filtered reads
```{r prep filtered sequences}
# vector of our samples, extract sample name from files

samples <- sapply(strsplit(basename(forward_reads),"_"), `[`,1)
#check
head(samples)

# prepare a placeholder for filtered reads
filtered_fastqs_path <- "/local/workdir/es986/git_repos/SalinityGradient_16S/data/01_DADA2/02_filtered_fastqs"

# create 2 variables for forward and reverse filtered

filtered_forward_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))
head(filtered_forward_reads)

filtered_reverse_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))
head(filtered_reverse_reads)

```



#filter and trim reads

Parameters of filter and trim  **depend on the dataset**
```{r Filter and Trim}
# prepare a placeholder for filtered reads

# maxN = number of N bases. Remove all Ns from data

#maxEE = quality filtering threshold applied to expected errors. Here is there's two expected errors it's ok. If more than two, throw away the sequence. Two values, first for fwd reads, second for rev reads

#trimLeft for the low quality base position

#multithread, compress helps computer

# Assign a vector to filtered reads
# Write out filtered fastq files
filtered_reads <- filterAndTrim(fwd = forward_reads, 
              filt = filtered_forward_reads, 
              rev = reverse_reads, filt.rev = filtered_reverse_reads,
              maxN = 0, maxEE = c(2,2), trimLeft = 3,
              truncQ = 2, rm.phix = TRUE, compress = TRUE)
              #multithread=TRUE for your personal project

#filterAndTrim()

```


# Trimmed Quality Plots
``` {r Filtrered Reads QC Plot}



# calculate quality and plot of these two samples
plotQualityProfile(filtered_forward_reads[random_samples])
  labs(title="Trimmed Forward Read Quality")

plotQualityProfile(filtered_reverse_reads[random_samples])
  labs(title="Trimmed Reverse Read Quality")
```

# Aggregated Trimmed Plots
```{r}
# Aggregate all plots - uncomment code and run on your own time
#plotQualityProfile(filtered_forward_reads, aggregate =TRUE) +
  #plotQualityProfile(filtered_reverse_reads, aggregate =TRUE)

```


#Stats on read output from filtereAndTrim

```{r}
filtered_df <- as.data.frame(filtered_reads)
head(filtered_reads)

# calculate some stats - fix this

filtered_df <-
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          meadian_percent_retained =(median(reads.out)/median(reads.in)))
```

# Error Modelling
```{r Learn Errors}
#Note: run separately on each illumina dataset

#Forward Reads
error_forward_reads <-
  learnErrors(filtered_forward_reads) #multithreaded = TRUE
#Plot Forward
plotErrors(error_forward_reads, nominalQ = TRUE) +
  labs(title= "Forward Read Error Model")

#Reverse Reads
error_reverse_reads <-
  learnErrors(filtered_reverse_reads) #multithreaded = TRUE

plotErrors(error_reverse_reads, nominalQ = TRUE) +
  labs(title= "Reverse Read Error Model")

```

# Infer ASVs
```{r Infer ASVs}
# Note that this is happening separately on the forward and reverse reads. This is unique to DADA2.

# infer forward ASVs
dada_forward <- dada(filtered_forward_reads,
                     err = error_forward_reads) # multithread=TRUE

# infer reverse ASVs
dada_reverse <- dada(filtered_reverse_reads,
                     err = error_reverse_reads) # multithread=TRUE

```


# Merge Forward and Reverse ASVs
```{r Merge ASVs}
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads,
                          dada_reverse, filtered_reverse_reads,
                          verbose=TRUE)

# Evaluate the output
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)

```


#Generate ASV Count Table
```{r Generate ASV Table }
# Create ASV Count Table

raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2


```

# Session Information
```{r Session Info}
# Ensure reproducibility
devtools::session_info()

```